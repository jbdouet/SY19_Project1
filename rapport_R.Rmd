---
title: "Rapport TP4: Regression et Classification"
output: pdf_document
subtitle: "Jean-Baptiste Douet | Louis Martignoni"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
load("env_R.Rdata")
library(MASS)
library(caret)
library(leaps)
```



Dans ce TP nous allons travailler sur deux problèmes: le premier de regression et le deuxième de classification. Le but est de trouver le meilleur prédicteur pour ces deux problèmes.  

## Régression linéaire

Nous disposons d'un jeu de données comportant 50 variables, nos prédicteurs. Le but sera donc de trouver quel modèle nous permettra de prédire le plus efficacement nos données. Pour cela nous alons entrainer nos modèles sur un échantillon d'entrainement pour ensuite faire nos prédictions sur notre echantillon de test.

Dans un premier nous allons faire une régression linéaire classique. Cependant, pour pouvoir comparer de manière la plus juste les différents modèles, nous avons décidé d'utiliser la Cross Validation et pour pouvoir avoir un estimateur non biaisé nous avons mis en place deux boucles imbriquées de Cross Validation. 


```{r}
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = n)) 
CV<-rep(0,10)
```
Nous commençons donc par mettre en place notre Validation Cross, on choisit de diviser notre jeu de données en 10 parties grâce à la fonction sample puis nous initialisons un vecteur nul de taille 10. A noter que la fonction sample va donc diviser de manière aléatoire ce qui ajoute un peu de volatilité dans nos calculs.

```{r}
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
  test_i <- which(folds_i == k)
  train_xy <- data_reg[-test_i, ]
  test_xy <- data_reg[test_i, ]
  fitControl <- trainControl(method = "cv",number = 10)
  model_lm <- caret::train(train_xy[,-51],train_xy$y,method='lm',trControl= fitControl)
  predictions_lm<-predict.train(object=model_lm,test_xy[,-51])
  CV[k]<- mean((test_xy$y-predictions_lm)^2)
}
```
En utilisant l'algorithme de la Cross Validation, nous créons donc nos differents compartiments de données, à chaque itération nous allons entrainer sur des compartiments et prédire sur un autre. fitControl nous permet de faire notre double Cross Validation. En effet, il va faire une Cross Validation lors de l'entrainement de notre modèle ainsi qu'optimiser si possible certains paramètres de ce dernier. On peut ensuite lancer notre prédiction sur le compartiment de test en se basant sur le modèle entrainé précedemment. Pour juger de la pertinence de notre modèle, nous devons calculer le MSE (mean squared error), c'est cette valeur que nous devons minimiser. Elle correspond à la moyenne des carrés de la difference entre nos valeurs de test et nos valeurs prédites à l'aide de notre modèle. 

```{r}
CVerror= sum(CV)/length(CV)
```

Pour évaluer au mieux notre MSE nous faisons enfin la moyenne des MSE trouvés à chaque itérations de notre boucle. C'est alors cette valeur qui va nous permettre de comparer l'efficacité des modèles entre eux. Pour ce premier modèle, il vaut en moyenne 99.

Après ce premier modèle nous avons tenté de l'améliorer avec le modèle Lasso. Nous allons procéder de la même manière que pour notre première régression lineaire mais cette fois ci notre modèle d'entrainement sera le modèle Lasso.

```{r, message=FALSE, warning=FALSE}
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = n)) 
CV<-rep(0,10)
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
  test_i <- which(folds_i == k)
  train_xy <- data_reg[-test_i, ]
  test_xy <- data_reg[test_i, ]
  fitControl <- trainControl(method = "cv",number = 10)
  model_lasso <- caret::train(train_xy[,-51],train_xy$y,method='lasso',trControl= fitControl)
  predictions_lasso<-predict.train(object=model_lasso,test_xy[,-51])
  CV[k]<- mean((test_xy$y-predictions_lasso)^2)
}
CVerror= sum(CV)/length(CV)

```

Ici nous avons amélioré notre CVerror puisqu'il vaut en moyenne 91.

Nous avons ensuite tenté d'améliorer ce résultat avec d'autres approches. La première étant de réaliser notre régression linéaire en ne sélectionnant que les prédicteurs les plus significatifs.

```{r}
reg <- lm(y~., data = data_reg.train)
sm <-summary(reg)
```

Grâce à summary(reg), nous pouvons identifier les prédicteurs les plus significatifs et ensuite réaliser une régression linéaire sur ces derniers. 

```{r}
lm.model<- lm(y~X4+X12+X19+X22+X24+X27+X35+X39+X41, data = data_reg.train)
pred2<-predict(lm.model, newdata = data_reg.test)
yi <- data_reg.test$y
errorBestPredictors<- mean((yi - pred2)^2)
```

Cependant cette approche est loin d'être concluante puisque notre MSE s'élève à 110. En diminuant le nombre de nos prédicteurs on a effectivement baisser notre variance mais notre modèle représente moins bien nos valeurs. 

Nous avons également tenté d'améliorer notre modèle en utilisant la technique du Best Subet en limitant à 15 prédicteurs. Cette technique vise à choisir un modèle utilisant l'ensemble contenant au maximum 15 prédicteurs le plus efficace possible. 

```{r}
#lm.fit<-regsubsets(y~.,data=data_reg.train,method='exhaustive', nvmax=15, really.big = T)
#plot(lm.fit,scale="r2")
```

Le graphique résultant (mais non exécuté pour des raisons d'espace) nous permet de déterminer le meilleur modèle contenant 15 prédicteurs. 

```{r}
lm_subset<- lm(y~X4+X6+X8+X12+X19+X20+X22+X24+X27+X30+X32+X35+X39+X41+X48, data = data_reg.train)
pred_sub<-predict(lm_subset, newdata = data_reg.test)
errorBestsubset <- mean((yi - pred_sub)^2)
```
Cependant le MSE de ce modèle est de 100 ce qui est encore une fois moins performant que notre modèle Lasso. On peut également remarquer que si on ne bride pas la valeur nvmax, alors cette technique choisira de prendre les 50 prédicteurs et donc nous rapprochera de la première regression classique. 
La technique du forward stepwise nous donne les mêmes résultats que le Best subset. Le modèle Lasso est donc toujours celui qui minimise notre espérance quadratique moyenne. 

Nous avons alors utilisé la méthode du PCR (Principal Component Regression).  On va donc entrainer notre modèle sur notre échantillon d'entrainement tout en réalisant une Cross Validation avec le paramètre fitControl.

```{r, message=FALSE, warning=FALSE}
fitControl <- trainControl(method = "cv",number = 10)
grid <- expand.grid(ncomp=c(3,5,10,20, 30,36,50))
model_pcr <- caret::train(data_reg.train[,-51],data_reg.train$y,method='pcr',trControl= fitControl,tuneGrid=grid)
```

Le Grid nous permet de visualiser en fonction de notre nombre de prédicteur, la qualité de notre espérance. 

```{r}
bestTuned <- model_pcr$bestTune
plot(model_pcr)
```

Nous pouvons ensuite avoir accès à des informations interessantes comme modelLookup qui nous informe que le paramètre ncomp (le nombre de prédicteurs) est celui qui peut le plus influencer positivement notre modèle et model_pcr$bestTune qui nous indique que la meilleure valeur de ncomp est 50. On peut bien visualiser ces informations dans notre plot si dessus qui nous montre que notre espérance est la plus faible pour ncomp à 50.

```{r}
predictions_pcr<-predict.train(object=model_pcr,data_reg.test[,-51],type="raw")
errorPcr <- mean((data_reg.test$y-predictions_pcr)^2)
```

Nous pouvons ensuite effectué nos prédictions sur notre échantillon de test en utilisant notre modèle entrainé. Mais encore une fois notre résultat n'est pas satisfaisant pour ce modèle puisque son MSE est de 110. 

Après avoir comparé les espérances quadratique moyennes de ces différents modèles, il en résulte que le meilleur modèle est le Lasso. 
On refait cette fois l'apprentissage sur le jeu de données complet.

```{r}
fitControl <- trainControl(method = "cv",number = 10)
regresseur <- caret::train(data_reg[,-51],data_reg$y,method='lasso',trControl= fitControl)
yhat=fitted(regresseur)
```

```{r}
plot(yhat,data_reg$y)
abline(0,1)  
```

Le plot ci-dessus nous montre la qualité de nos prédictions en utilisant le modèle Lasso. On remarque très bien la relation linéaire entre nos valeurs et nos prédictions.
On décide donc ensuite de tracer nos résidus pour valider notre modèle.

```{r}
rres <- resid(regresseur)
plot(yhat,rres)
```

Notre plot ci-dessus nous montre que nos résidus sont bien centrés autour de 0 et ils ne montrent pas de régularité ce qui montre bien leur indépendance.

```{r}
qqnorm(rres)
qqline(rres, dist = qnorm)
```

On vérifie avec cette dernière courbe que les residus suivent bien une loi normale.
Comme test supplémentaire, on pourra aussi vérifier les points aberrants (on ne le fera pas ici par économie de place).

## Classifieur 

Nous disposons cette fois d'un jeu de données comportant 30 variables. 

Comme dans la partie précédente nous allons essayer de trouver le meilleur modèle possible en testant différente technique et en les évaluant avec une double validation croisée. 

Nous commençons par étudier les données:
```{r}
# Missing values
sum(is.na(data_clas.train)) # =0
sum(is.na(data_clas.test)) # =0

### Balance classes

dim(data_clas[data_clas$y ==1,]) # 77 elements 
dim(data_clas[data_clas$y ==2,]) # 123 elements 
```

On peut voir qu'il n'y a pas de valeur manquantes dans les données. Et les classes ne sont pas équilibrées, on a quasiment un rapport de 2 entre les deux classes, la validation croisée sera d'autant plus importante pour la validation et le choix du modèle optimal.
 
Nous avons testé différents modèles: LDA, SVM, Regression logistique (avec et sans régularisation) ainsi que le modèle Naive Bayes.

Pour les méthodes d'optimisation, nous avons travailler avec le PCA ainsi qu'avec des méthodes de régularisation pour certains modèles. 
 
Les deux modèles qui ont été les plus convaincants sont le Naive Bayes et la régression logistique régularisée. Leur évaluation s'est faire avec la double validation croisée.
```{r}
data_clas$y=as.factor(data_clas$y) 
data_clas.train$y=as.factor(data_clas.train$y) 
data_clas.test$y=as.factor(data_clas.test$y)
```

```{r, warning=FALSE}
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = ntrain)) 
CV<-rep(0,10)
for (k in 1:n_folds) {
  test_i <- which(folds_i == k)
  train_xy <- data_clas[-test_i, ]
  test_xy <- data_clas[test_i, ]
  model_naive_bayes <- train(train_xy[,-31],train_xy$y,method='nb',trControl= trainControl(
    method = "cv",
    number =10,
    verboseIter = FALSE))
  predictions_naive_bayes<-predict.train(object=model_naive_bayes,test_xy[,-31])
  cf<-confusionMatrix(predictions_naive_bayes,test_xy$y) 
  CV[k]<- cf$overall["Accuracy"]
}
CVerror= sum(CV)/length(CV)
```



Le modèle nous donne une accuracy moyenne sur les cross validation de 0,76.

Le second modèle est la régression logistique régularisée:

```{r}

n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = ntrain)) 
CV<-rep(0,10)
for (k in 1:n_folds) {
  test_i <- which(folds_i == k)
  train_xy <- data_clas[-test_i, ]
  test_xy <- data_clas[test_i, ]
  model_regLogistic<- train(train_xy[,-31],train_xy$y,method='regLogistic',trControl= trainControl(
    method = "cv",
    number =10,
    verboseIter = FALSE))
  predictions_regLogistic<-predict.train(object=model_regLogistic,test_xy[,-31])
  cf<-confusionMatrix(predictions_regLogistic,test_xy$y) 
  CV[k]<- cf$overall["Accuracy"]
}
CVerror= sum(CV)/length(CV)

```

Les résultats sont similaires à celui du Naive Bayes quoique légèrement meilleurs, en effet on obtient une accuracy moyenne sur les cross validation de 0,77.
L'intérêt de la régularisation est de permettre au modèle de mieux généraliser et d'éviter les situations d'overfit.
C'est avec ce modèle que l'on construira notre ficher classifieur.

Afin d'améliorer nos prédictions nous avons implémenter le PCA en prenant suffisament de composants afin de capturer 90% de la variance ce qui correspond à 24 composants.
Malgré cela, utiliser ces composants en vecteur d'entrée des algorithmes testés n'a pas amélioré nos résultats.
De ce fait, nous n'avons pas retenu le PCA car sans amélioration de résultat, il nuisait à l'interprétation de nos données.


## Pour aller plus loin 

Pour finir nous pouvons évoquer quelques techniques qui, nous pensons, auraient pû contribuer à améliorer nos prédictions. 
Tout d'abord, pour sélectioner les meilleurs subsets ainsi que les hyperparamètres les techniques de grid search et de genetic algorithms nous auraient permis d'affiner nos prédictions tout en gardant les mêmes modèles.

Une autre solution aurait pu être de combiner différents modèles avec des techniques telles que le boosting et le bagging. 


