---
title: "Rapport TP4: Regression et Classification"
output: pdf_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
load("SY19_Project1/env_R.Rdata")
library(MASS)
library(caret)
library(leaps)
```



Dans ce TP nous allons travailler sur deux problèmes: le premier de regression et le deuxième de classification. Le but est de trouver le meilleur prédicteur pour ces deux problèmes.  

## Régression linéaire

Nous disposons d'un jeu de données comportant 50 variables, nos préditeurs. Le but sera donc de trouver quel modèle nous permettra de prédire le plus efficacement nos données. Pour cela nous alons entrainer nos modèles sur un echantillon d'entrainement pour ensuite faire nos prédictions sur notre echantillon de test.

Dans un premier nous allons faire une régression linéaire classique. Cependant, pour pouvoir comparer de manière la plus juste les différents modèles, nous avons décidé d'utiliser la Cross Validation et pour pouvoir avoir un estimateur non biaisé nous avons mis en place deux boucles imbriquées de Cross Validation. 


```{r}
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = n)) 
table(folds_i)
CV<-rep(0,10)
```
Nous commençons donc par mettre en place notre Validation Cross, on choisit de diviser notre jeu de données en 10 parties grâce à la fonction sample puis nous initialisons un vecteur nul de taille 10. A noter que la fonction sample va donc diviser de manière aléatoire ce qui ajoute un peu de volatilité dans nos calculs.

```{r}
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
  test_i <- which(folds_i == k)
  train_xy <- data_reg[-test_i, ]
  test_xy <- data_reg[test_i, ]
  fitControl <- trainControl(method = "cv",number = 10)
  model_lm <- caret::train(train_xy[,-51],train_xy$y,method='lm',trControl= fitControl)
  predictions_lm<-predict.train(object=model_lm,test_xy[,-51])
  CV[k]<- mean((test_xy$y-predictions_lm)^2)
}
```
En utilisant l'algorithme de la Cross Validation, nous créons donc nos differents compartiments de données, à chaque itération nous allons entrainer sur des compartiments et prédire sur un autre. fitControl nous permet de faire notre double Cross Validation. En effet, il va faire une Cross Validation lors de l'entrainement de notre modèle ainsi qu'optimiser si possible certains paramètres de ce dernier. On peut ensuite lancer notre prédiction sur le compartiment de test en se basant sur le modèle entrainé précedemment. Pour juger de la pertinence de notre modèle, nous devons calculer le MSE (mean squared error), c'est cette valeur que nous devons minimiser. Elle correspond à la moyenne des carrés de la difference entre nos valeurs de test et nos valeurs prédites à l'aide de notre modèle. 

```{r}
CVerror= sum(CV)/length(CV)
```

Pour évaluer au mieux notre MSE nous faisons enfin la moyenne des MSE trouvés à chaque itérations de notre boucle. C'est alors cette valeur qui va nous permettre de comparer l'efficacité des modèles entre eux. Pour ce premier modèle, il vaut en moyenne 99.

Après ce premier modèle nous avons tenté de l'améliorer avec le modèle Lasso. Nous allons procéder de la même manière que pour notre première régression lineaire mais cette fois ci notre modèle d'entrainement sera le modèle Lasso.

```{r}
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = n)) 
table(folds_i)
CV<-rep(0,10)
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
  test_i <- which(folds_i == k)
  train_xy <- data_reg[-test_i, ]
  test_xy <- data_reg[test_i, ]
  fitControl <- trainControl(method = "cv",number = 10)
  model_lasso <- caret::train(train_xy[,-51],train_xy$y,method='lasso',trControl= fitControl)
  predictions_lasso<-predict.train(object=model_lasso,test_xy[,-51])
  CV[k]<- mean((test_xy$y-predictions_lasso)^2)
}
CVerror= sum(CV)/length(CV)
```

Ici nous avons amélioré notre CVerror puisqu'il vaut en moyenne 92.

Nous avons ensuite tenté d'améliorer ce résultat avec d'autres approches. La première étant de réaliser notre régression linéaire en ne sélectionnant que les prédicteurs les plus significatifs.

```{r}
reg <- lm(y~., data = data_reg.train)
sm <-summary(reg)
```

Grâce à summary(reg), nous pouvons identifier les prédicteurs les plus significatifs et ensuite réaliser une régression linéaire sur ces derniers. 

```{r}
lm.model<- lm(y~X4+X12+X19+X22+X24+X27+X35+X39+X41, data = data_reg.train)
pred2<-predict(lm.model, newdata = data_reg.test)
yi <- data_reg.test$y
mean((yi - pred2)^2)
```

Cependant cette approche est loin d'être concluante puisque notre MSE s'élève à 110. En diminuant le nombre de nos prédicteurs on a effectivement baisser notre variance mais notre modèle représente moins bien nos valeurs. 

Nous avons également tenté d'améliorer notre modèle en utilisant la technique du Best Subet en limitant à 15 prédicteurs. Cette technique vise à choisir un modèle utilisant l'ensemble contenant au maximum 15 prédicteurs le plus efficace possible. 

```{r}
lm.fit<-regsubsets(y~.,data=data_reg.train,method='exhaustive', nvmax=15, really.big = T)
plot(lm.fit,scale="r2")
```

Ce graphique nous permet de déterminer le meilleur modèle contenant 15 prédicteurs. 

```{r}
lm_subset<- lm(y~X4+X6+X8+X12+X19+X20+X22+X24+X27+X30+X32+X35+X39+X41+X48, data = data_reg.train)
pred_sub<-predict(lm_subset, newdata = data_reg.test)
mean((yi - pred_sub)^2)
```
Cependant le MSE de ce modèle est de 100 ce qui est encore une fois moins performant que notre modèle Lasso. On peut également remarquer que si on ne bride pas la valeur nvmax, alors cette technique choisira de prendre les 50 prédicteurs et donc nous rapprochera de la première regression classique. 
La technique du forward stepwise nous donne les mêmes résultats que le Best subset. Le modèle Lasso est donc toujours celui qui minimise notre espérance quadratique moyenne. 

Nous avons alors utilisé la méthode du PCR (Principal Component regression). Le but de cette technique est donc de réduire le nombre de prédicteurs pour diminuer notre variance et donc améliorer notre espérance quadratique moyenne. On va donc entrainer notre modèle sur notre échantillon d'entrainement tout en réalisant une Cross Validation avec le paramètre fitControl.

```{r}
fitControl <- trainControl(method = "cv",number = 10)
grid <- expand.grid(ncomp=c(3,5,10,20, 30,36,50))
model_pcr <- caret::train(data_reg.train[,-51],data_reg.train$y,method='pcr',trControl= fitControl,tuneGrid=grid)
```

Le Grid nous permet de visualiser en fonction de notre nombre de prédicteur, la qualité de notre espérance. 

```{r}
model_pcr$bestTune
modelLookup(model='pcr')
plot(model_pcr)
```

Nous pouvons ensuite avoir accès à des informations interessantes comme modelLookup qui nous informe que le paramètre ncomp (le nombre de prédicteurs) est celui qui peut le plus influencer positivement notre modèle et model_pcr$bestTune qui nous indique que la meilleure valeur de ncomp est 50. On peut bien visualiser ces informations dans notre plot si dessus qui nous montre que notre espérance est la plus faible pour ncomp à 50.

```{r}
plot(varImp(object=model_pcr),main="PCR - Variable Importance")
```
 
Ce graphique nous montre l'ordre d'importance de nos prédicteurs dans notre PCR.

```{r}
predictions_pcr<-predict.train(object=model_pcr,data_reg.test[,-51],type="raw")
mean((data_reg.test$y-predictions_pcr)^2)
```

Nous pouvons ensuite effectuer nos prédictions sur notre échantillon de test en utilisant notre modèle entrainé. Mais encore une fois notre résultat n'est pas satisfaisant pour ce modèle puisque son MSE est de 108. 

```{r}
plot(predictions_pcr,data_reg.test$y)
abline(0,1)
```

Ce plot nous montre bien que notre modèle permet de prédire globalement nos valeurs mais pas de manière suffisament efficace pour être retenu.

Après avoir comparé les espérances quadratique moyennes de ces différents modèles, il en résulte que le meilleur modèle est le Lasso. 

```{r}
fitControl <- trainControl(method = "cv",number = 10)
model_lasso <- caret::train(data_reg.train[,-51],data_reg.train$y,method='lasso',trControl= fitControl)
predictions_lasso<-predict.train(object=model_lasso,data_reg.test[,-51])
plot(predictions_lasso,data_reg.test$y)
abline(0,1)  
```

Le plot ci-dessus nous montre la qualité de nos prédictions en utilisant le modèle Lasso. On remarque très bien la relation linéaire entre nos valeurs et nos prédictions.
On décide donc ensuite de tracer nos résidus pour valider notre modèle.

```{r}
rres <- resid(model_lasso)
plot(rres)
```

Notre plot ci-dessus nous montre que nos résidus sont bien centrés autour de 0 et ils ne montrent pas de régularité ce qui montre bien leur indépendance.

```{r}
plot(rstandard(model_lasso))
```








You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
