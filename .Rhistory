#cv_tmp[k, ] <- sapply(as.list(data.frame(pred)), function(y_hat) mean((y -  y_hat)^2))
}
for (k in 1:n_folds) {
test_i <- which(folds_i == k)
train_xy <- data_clas2.train[-test_i, ]
test_xy <- data_clas2.train[test_i, ]
#x <- train_xy
y <- train_xy$y
glm.fit<- glm(y~.,data=train_xy,family=binomial)
#fitted_models <- apply(t(df), 2, function(degf) lm(y ~ ns(x, df = degf)))
#x <- test_xy$x
y <- test_xy$y
cv_pred<-predict(glm.fit,newdata=test_xy)
#pred <- mapply(function(obj, degf) predict(obj, data.frame(ns(x, df = degf))), fitted_models, df)
dim(cv_pred)
#cv_tmp[k, ] <- sapply(as.list(data.frame(pred)), function(y_hat) mean((y -  y_hat)^2))
}
dim(cv_pred)
CV<-rep(0,10)
for(i in (1:1)){ # this is to apply the cross validation many times
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
train_xy <- data_clas2.train[-test_i, ]
test_xy <- data_clas2.train[test_i, ]
print(k)
glm.fit<- glm(y~.,data=train_xy,family=binomial)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
cv_pred<-predict(glm.fit,newdata=data_clas2.train[folds==k,])
cv_pred
dim(cv_pred)
CV[i]<-CV[i]+ sum((data_clas2.train$y[folds==k]-cv_pred)^2)
}
CV[i]<-CV[i]/n
}
warnings()
data_clas2.train  <- data_clas.train
data_clas2.test  <- data_clas.test
data_clas2.train$y <- data_clas.train$y-1
data_clas2.test$y <- data_clas.test$y-1
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = ntrain)) # !!! le ntrain doit correspondre à la taille du dataset que l'on utilisera dans la boucle de cross validation
table(folds_i) # Pas le même nombre d'éléments
CV<-rep(0,10)
for(i in (1:1)){ # this is to apply the cross validation many times
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
train_xy <- data_clas2.train[-test_i, ]
test_xy <- data_clas2.train[test_i, ]
print(k)
glm.fit<- glm(y~.,data=train_xy,family=binomial)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
cv_pred<-predict(glm.fit,newdata=data_clas2.train[folds==k,])
cv_pred
dim(cv_pred)
CV[i]<-CV[i]+ sum((data_clas2.train$y[folds==k]-cv_pred)^2)
}
CV[i]<-CV[i]/n
}
warnings()
for(i in (1:1)){ # this is to apply the cross validation many times
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
train_xy <- data_clas2.train[-test_i, ]
test_xy <- data_clas2.train[test_i, ]
print(k)
glm.fit<- glm(y~.,data=train_xy,family=binomial)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
cv_pred<-predict(glm.fit,newdata=test_xy, type = "response")
dim(cv_pred)
CV[i]<-CV[i]+ sum((data_clas2.train$y[folds==k]-cv_pred)^2)
}
CV[i]<-CV[i]/n
}
print(dim(cv_pred))
for(i in (1:1)){ # this is to apply the cross validation many times
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
train_xy <- data_clas2.train[-test_i, ]
test_xy <- data_clas2.train[test_i, ]
print(k)
glm.fit<- glm(y~.,data=train_xy,family=binomial)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
cv_pred<-predict(glm.fit,newdata=test_xy, type = "response")
print(dim(cv_pred))
CV[i]<-CV[i]+ sum((data_clas2.train$y[folds==k]-cv_pred)^2)
}
CV[i]<-CV[i]/n
}
for(i in (1:1)){ # this is to apply the cross validation many times
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
train_xy <- data_clas2.train[-test_i, ]
test_xy <- data_clas2.train[test_i, ]
print(k)
glm.fit<- glm(y~.,data=train_xy,family=binomial)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
cv_pred<-predict(glm.fit,newdata=test_xy, type = "response")
print(length(cv_pred))
CV[i]<-CV[i]+ sum((data_clas2.train$y[folds==k]-cv_pred)^2)
}
CV[i]<-CV[i]/n
}
CV[1]
for(i in (1:1)){ # this is to apply the cross validation many times
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
train_xy <- data_clas2.train[-test_i, ]
test_xy <- data_clas2.train[test_i, ]
print(k)
glm.fit<- glm(y~.,data=train_xy,family=binomial)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
cv_pred<-predict(glm.fit,newdata=test_xy, type = "response")
print(length(cv_pred))
CV[i]< sum((test_xy$y-cv_pred)^2)
}
#CV[i]<-CV[i]/n
}
CV
for(i in (1:1)){ # this is to apply the cross validation many times
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
train_xy <- data_clas2.train[-test_i, ]
test_xy <- data_clas2.train[test_i, ]
print(k)
glm.fit<- glm(y~.,data=train_xy,family=binomial)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
cv_pred<-predict(glm.fit,newdata=test_xy, type = "response")
print(length(cv_pred))
CV[k]< sum((test_xy$y-cv_pred)^2)
}
#CV[i]<-CV[i]/n
}
CV
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
train_xy <- data_clas2.train[-test_i, ]
test_xy <- data_clas2.train[test_i, ]
print(k)
glm.fit<- glm(y~.,data=train_xy,family=binomial)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
cv_pred<-predict(glm.fit,newdata=test_xy, type = "response")
CV[k]< sum((test_xy$y-cv_pred)^2)
}
}
CV
for(i in (1:1)){ # this is to apply the cross validation many times
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
train_xy <- data_clas2.train[-test_i, ]
test_xy <- data_clas2.train[test_i, ]
print(k)
glm.fit<- glm(y~.,data=train_xy,family=binomial)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
cv_pred<-predict(glm.fit,newdata=test_xy, type = "response")
CV[k]< sum((test_xy$y-cv_pred)^2)
}
#CV[i]<-CV[i]/n
}
CV
train_xy <- data_clas2.train[-test_i, ]
for(i in (1:1)){ # this is to apply the cross validation many times
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
train_xy <- data_clas2.train[-test_i, ]
test_xy <- data_clas2.train[test_i, ]
print(k)
glm.fit<- glm(y~.,data=train_xy,family=binomial)
cv_pred<-predict(glm.fit,newdata=test_xy, type = "response")
CV[k]<- sum((test_xy$y-cv_pred)^2)
}
#CV[i]<-CV[i]/n
}
CV
CVerror= sum(CV)
CVerror= sum(CV)/length(CV)
install.packages("ggplot2")
install.packages("ggplot2")
data_clas_scaled.test<-scale(data_clas[-train,])
View(data_clas_scaled.test)
View(data_clas.test)
View(data_clas_scaled.test)
View(data_clas_scaled.test)
data_clas_scaled.train<-data_clas[train,]
data_clas_scaled.train$y
data_clas_scaled.train$X1
data_clas.train$X1
data_clas_scaled.train$X1==data_clas.train$X1
data_clas_scaled.train$X2==data_clas.train$X2
set.seed(42)
install.packages("caret")
help("~")
data_clas_scaled.train==data_clas.train
preProcValues <- preProcess(data_clas.train, method = c("center", "scale"))
require(caret)
data_clas_scaled.train==data_clas.train
preProcValues <- preProcess(data_clas.train, method = c("center", "scale"))
preProcValues$mean
data_clas.train$y=as.factor(y)
data_clas.train$y=as.factor(data_clas.train$y)
preProcValues <- preProcess(data_clas.train, method = c("center", "scale"))
preProcValues$mean
preProcValues$data
data_clas.train$y=as.factor(data_clas.train$y)
data_clas_scaled.train<-scale(data_clas[train,])
data_clas_scaled.train==data_clas.train
require(caret)
data_clas_scaled.test<-scale(data_clas[-train,])
data_clas_scaled.train<-scale(data_clas[train,])
View(data_clas_scaled.train)
data_clas_scaled.test<-scale(data_clas.test)
data_clas_scaled.train<-scale(data_clas.train)
data_clas_scaled.test=as.data.frame(data_clas_scaled.test)
data_clas_scaled.train<-scale(data_clas.train)
data_clas_scaled.test<-scale(data_clas.test)
data_clas_scaled.test=as.data.frame(data_clas_scaled.test)
data_clas_scaled.train[] <- lapply(data_clas.train, function(x) if(is.numeric(x)){
scale(x, center=TRUE, scale=TRUE)
} else x)
data_clas_scaled.train[] <- lapply(data_clas.train, function(x) if(is.numeric(x)){
scale(x, center=TRUE, scale=TRUE)
} else x)
data_clas_scaled.train<- lapply(data_clas.train, function(x) if(is.numeric(x)){
scale(x, center=TRUE, scale=TRUE)
} else x)
data_clas_scaled.train=as.data.frame(data_clas_scaled.train)
View(data_clas.train)
help("lapply")
data_clas_scaled.test<- lapply(data_clas.test, function(x) if(is.numeric(x)){
scale(x, center=TRUE, scale=TRUE)
} else x)
data_clas_scaled.test=as.data.frame(data_clas_scaled.test)
help("requie")
help("require")
sum(is.na(data_clas.train))
sum(is.na(data_clas.test))
names(getModelInfo())
help("train")
data_clas_scaled.train[,-y]
data_clas_scaled.train[,y]
View(data_clas_scaled.train)
data_clas_scaled.train$y
data_clas_scaled.train$-y
data_clas_scaled.train[,-1]
data_clas_scaled.train[,-31]
model_glm <- train(data_clas_scaled.train[,-31],data_clas_scaled.train$y,method='glm',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
plot(model_glm)
modelLookup(model='glm')
modelLookup(model='glm')
modelLookup(model='gbm')
model_glm <- train(data_clas_scaled.train[,-31],data_clas_scaled.train$y,method='glm',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE),
tuneLength=10))
model_glm <- train(data_clas_scaled.train[,-31],data_clas_scaled.train$y,method='glm',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE),
tuneLength=10)
plot(model_glm)
plot(varImp(object=model_glm),main="NNET - Variable Importance")
predictions<-predict.train(object=model_gbm,testSet[,predictors],type="raw")
predictions<-predict.train(object=model_glm,data_clas_scaled.test[,-31],type="raw")
table(predictions)
confusionMatrix(predictions,data_clas_scaled.test$y)
data_clas_scaled.test$y
data_clas_scaled.test<- lapply(data_clas.test, function(x) if(is.numeric(x)){
scale(x, center=TRUE, scale=TRUE)
} else x)
data_clas_scaled.test=as.data.frame(data_clas_scaled.test)
data_clas_scaled.test$y
data_clas_scaled.train$y
data_clas.test$y=as.factor(data_clas.test$y)
data_clas_scaled.test<- lapply(data_clas.test, function(x) if(is.numeric(x)){
scale(x, center=TRUE, scale=TRUE)
} else x)
data_clas_scaled.test=as.data.frame(data_clas_scaled.test)
data_clas_scaled.test$y
confusionMatrix(predictions,data_clas_scaled.test$y)
names(getModelInfo()) # donne le nom des modeles
model_xgbTree <- train(data_clas_scaled.train[,-31],data_clas_scaled.train$y,method='xgbTree',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
names(getModelInfo()) # donne le nom des modeles
model_rf <- train(data_clas_scaled.train[,-31],data_clas_scaled.train$y,method='rf',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
plot(model_rf)# does not work if no parameters to tune
plot(varImp(object=model_rf),main="rf - Variable Importance")
predictions<-predict.train(object=model_rf,data_clas_scaled.test[,-31],type="raw")
table(predictions)
confusionMatrix(predictions,data_clas_scaled.test$y)
help("nnet")
help("neuralnet")
names(getModelInfo()) # donne le nom des modeles
help("neuralnet")
model_rf <- train(data_clas_scaled.train[,-31],data_clas_scaled.train$y,method='nnet',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
modelLookup(model='nnet') # no parameters to tune
plot(model_nnet)# does not work if no parameters to tune
model_nnet <- train(data_clas_scaled.train[,-31],data_clas_scaled.train$y,method='nnet',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
modelLookup(model='nnet') # no parameters to tune
plot(model_nnet)# does not work if no parameters to tune
plot(varImp(object=model_nnet),main="nnet - Variable Importance")
plot(varImp(object=model_nnet),main="nnet - Variable Importance")
plot(model_nnet)# does not work if no parameters to tune
predictions<-predict.train(object=model_nnet,data_clas_scaled.test[,-31],type="raw")
table(predictions)
confusionMatrix(predictions,data_clas_scaled.test$y)
model_nnet <- train(data_clas.train[,-31],data_clas.train$y,method='nnet',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
modelLookup(model='nnet') # no parameters to tune
plot(varImp(object=model_nnet),main="nnet - Variable Importance")
predictions<-predict.train(object=model_nnet,data_clas_scaled.test[,-31],type="raw")
table(predictions)
confusionMatrix(predictions,data_clas_scaled.test$y)
predictions<-predict.train(object=model_nnet,data_clas.test[,-31],type="raw")
table(predictions)
confusionMatrix(predictions,data_clas.test$y)
16+28
44/66
names(getModelInfo()) # donne le nom des modeles
help(nb)
??nb
model_naive_bayes <- train(data_clas.train[,-31],data_clas.train$y,method='naive_bayes',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
modelLookup(model='naive_bayes') # no parameters to tune
plot(model_naive_bayes)# does not work if no parameters to tune
plot(varImp(object=model_naive_bayes),main="naive_bayes - Variable Importance")
predictions_naive_bayes<-predict.train(object=model_naive_bayes,data_clas.test[,-31],type="raw")
table(predictions_naive_bayes)
confusionMatrix(predictions_naive_bayes,data_clas.test$y)
data_clas[y==1]
dim(data_clas[y==1])
dim(data_clas[y==2])
dim(data_clas[data_class$y==1])
dim(data_clas[data_clas$y==1])
dim(data_clas[y==1])
dim(data_clas[y=1])
dim(data_clas[y!=1])
dim(data_clas[data_class$y!=1])
dim(data_clas[data_clas$y!=1])
dim(data_clas$y!=1)
data_clas[data_clas$y ==1,]
dim(data_clas[data_clas$y ==1,])
dim(data_clas[data_clas$y ==2,])
model_naive_bayes <- train(data_clas.train[,-31],data_clas.train$y,method='naive_bayes',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
modelLookup(model='naive_bayes') # no parameters to tune
plot(model_naive_bayes)# does not work if no parameters to tune
plot(varImp(object=model_naive_bayes),main="naive_bayes - Variable Importance")
predictions_naive_bayes<-predict.train(object=model_naive_bayes,data_clas.test[,-31],type="raw")
table(predictions_naive_bayes)
confusionMatrix(predictions_naive_bayes,data_clas.test$y)
model_naive_bayes <- train(data_clas.train[,-31],data_clas.train$y,method='naive_bayes',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
modelLookup(model='naive_bayes') # no parameters to tune
plot(model_naive_bayes)# does not work if no parameters to tune
plot(varImp(object=model_naive_bayes),main="naive_bayes - Variable Importance")
predictions_naive_bayes<-predict.train(object=model_naive_bayes,data_clas.test[,-31],type="raw")
table(predictions_naive_bayes)
confusionMatrix(predictions_naive_bayes,data_clas.test$y)
model_naive_bayes <- train(data_clas.train[,-31],data_clas.train$y,method='naive_bayes',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
modelLookup(model='naive_bayes') # no parameters to tune
plot(model_naive_bayes)# does not work if no parameters to tune
plot(varImp(object=model_naive_bayes),main="naive_bayes - Variable Importance")
predictions_naive_bayes<-predict.train(object=model_naive_bayes,data_clas.test[,-31],type="raw")
table(predictions_naive_bayes)
confusionMatrix(predictions_naive_bayes,data_clas.test$y) # accuracy =0.7424
model_naive_bayes <- train(data_clas.train[,-31],data_clas.train$y,method='naive_bayes',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
modelLookup(model='naive_bayes') # no parameters to tune
plot(model_naive_bayes)# does not work if no parameters to tune
plot(varImp(object=model_naive_bayes),main="naive_bayes - Variable Importance")
predictions_naive_bayes<-predict.train(object=model_naive_bayes,data_clas.test[,-31],type="raw")
table(predictions_naive_bayes)
confusionMatrix(predictions_naive_bayes,data_clas.test$y) # accuracy =0.7424
model_naive_bayes <- train(data_clas_scaled.train[,-31],data_clas.train$y,method='naive_bayes',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
modelLookup(model='naive_bayes') # no parameters to tune
plot(model_naive_bayes)# does not work if no parameters to tune
predictions_naive_bayes<-predict.train(object=model_naive_bayes,data_clas_scaled.test[,-31],type="raw")
table(predictions_naive_bayes)
confusionMatrix(predictions_naive_bayes,data_clas.test$y) # accuracy =0.7424
model_naive_bayes <- train(data_clas_scaled.train[,-31],data_clas.train$y,method='naive_bayes',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
modelLookup(model='naive_bayes') # no parameters to tune
plot(model_naive_bayes)# does not work if no parameters to tune
plot(varImp(object=model_naive_bayes),main="naive_bayes - Variable Importance")
predictions_naive_bayes<-predict.train(object=model_naive_bayes,data_clas_scaled.test[,-31],type="raw")
table(predictions_naive_bayes)
confusionMatrix(predictions_naive_bayes,data_clas.test$y) # accuracy =0.7424
model_naive_bayes <- train(data_clas_scaled.train[,-31],data_clas.train$y,method='naive_bayes',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
modelLookup(model='naive_bayes') # no parameters to tune
plot(model_naive_bayes)# does not work if no parameters to tune
plot(varImp(object=model_naive_bayes),main="naive_bayes - Variable Importance")
predictions_naive_bayes<-predict.train(object=model_naive_bayes,data_clas_scaled.test[,-31],type="raw")
table(predictions_naive_bayes)
confusionMatrix(predictions_naive_bayes,data_clas.test$y) # accuracy =0.7424
model_svmPoly <- train(data_clas_scaled.train[,-31],data_clas.train$y,method='svmPoly',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
modelLookup(model='svmPoly') # no parameters to tune
plot(model_svmPoly)# does not work if no parameters to tune
plot(varImp(object=model_svmPoly),main="svmPoly - Variable Importance")
predictions_svmPoly<-predict.train(object=model_svmPolys,data_clas_scaled.test[,-31],type="raw")
table(predictions_svmPoly)
confusionMatrix(predictions_svmPoly,data_clas.test$y) # accuracy =0.7424
predictions_svmPoly<-predict.train(object=model_svmPoly,data_clas_scaled.test[,-31],type="raw")
table(predictions_svmPoly)
confusionMatrix(predictions_svmPoly,data_clas.test$y) # accuracy =0.7424
plot(varImp(object=model_naive_bayes),main="naive_bayes - Variable Importance")
plot(varImp(object=model_naive_bayes),main="naive_bayes - Variable Importance")
predictions_naive_bayes<-predict.train(object=model_naive_bayes,data_clas_scaled.test[,-31],type="raw")
table(predictions_naive_bayes)
confusionMatrix(predictions_naive_bayes,data_clas.test$y) # accuracy =0.7424
model_svmPoly <- train(data_clas_scaled.train[,-31],data_clas.train$y,method='svmPoly',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
modelLookup(model='svmPoly') # no parameters to tune
plot(model_svmPoly)# does not work if no parameters to tune
plot(varImp(object=model_svmPoly),main="svmPoly - Variable Importance")
predictions_svmPoly<-predict.train(object=model_svmPoly,data_clas_scaled.test[,-31],type="raw")
table(predictions_svmPoly)
confusionMatrix(predictions_svmPoly,data_clas.test$y)
model_svmPoly <- train(data_clas_scaled.train[,-31],data_clas.train$y,method='pcr',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
model_mlpML <- train(data_clas_scaled.train[,-31],data_clas.train$y,method='mlpML',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
modelLookup(model='mlpML') # no parameters to tune
plot(model_mlpML)# does not work if no parameters to tune
table(predictions_mlpML)
predictions_mlpML<-predict.train(object=model_mlpML,data_clas_scaled.test[,-31],type="raw")
table(predictions_mlpML)
confusionMatrix(predictions_svmPoly,data_clas.test$y)
confusionMatrix(predictions_mlpML,data_clas.test$y)
confusionMatrix(predictions_nnet,data_clas.test$y)
confusionMatrix(predictions_svmPoly,data_clas.test$y)
table(predictions_svmPoly)
confusionMatrix(predictions_svmPoly,data_clas.test$y)
table(predictions_mlpML)
confusionMatrix(predictions_mlpML,data_clas.test$y)
predictions_mlpML<-predict.train(object=model_mlpML,data_clas_scaled.test[,-31],type="raw")
table(predictions_mlpML)
predictions_nnet<-predict.train(object=model_nnet,data_clas.test[,-31],type="raw")
table(predictions_nnet)
confusionMatrix(predictions_nnet,data_clas.test$y)
model_naive_bayes <- train(data_clas_scaled.train[,-31],data_clas.train$y,method='naive_bayes',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
modelLookup(model='naive_bayes') # no parameters to tune
plot(model_naive_bayes)# does not work if no parameters to tune
plot(varImp(object=model_naive_bayes),main="naive_bayes - Variable Importance")
predictions_naive_bayes<-predict.train(object=model_naive_bayes,data_clas_scaled.test[,-31],type="raw")
table(predictions_naive_bayes)
confusionMatrix(predictions_naive_bayes,data_clas.test$y) # accuracy =0.7424
model_svmPoly <- train(data_clas_scaled.train[,-31],data_clas.train$y,method='svmPoly',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
model_svmPoly <- train(data_clas_scaled.train[,-31],data_clas.train$y,method='svmPoly',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
